{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4849f3-51e1-4299-9f55-9c70c1f1c431",
   "metadata": {},
   "source": [
    "# Fine-Tuning Models\n",
    "> Fine-tuning using your own data\n",
    "\n",
    "In this notebook, we'll use two references:https://huggingface.co/transformers/custom_datasets.html as a guide for our work.  We'll use the HuggingFace dataset we've already created and use it directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbc1b7-f4b4-4cdd-93ed-ce92742fe4c6",
   "metadata": {},
   "source": [
    "### Install required packages\n",
    "Note that this is mostly required if you're on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067752c-e281-4ffe-9140-828d158751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers\n",
    "#! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4f46c-e1c0-4abc-9498-a45211bcf9a6",
   "metadata": {},
   "source": [
    "### Import packages of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa3949-c286-4a82-af4f-c5f7594a6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902fa8d-8192-4ab0-8f5f-9a0ddf6c34fd",
   "metadata": {},
   "source": [
    "# 0. Log into HuggingFace CLI\n",
    "Why are we doing this? Below, we'll use our own user accounts to grab datasets and upload models. If we don't do this, we'll have to pass in the auth token over. This isn't bad, but let's streamline our efforts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75fd3d-76d2-425f-be19-aace4db0e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b6acb-bf9e-4e5c-8764-90eed505a2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f8c38bcd6c400ba9d2045aafc06dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a371ec-98f5-4be7-b085-a2eed2f9c3d3",
   "metadata": {},
   "source": [
    "# 1. Load data from HuggingFace Hub or from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b6693-573c-4515-b3bc-0a70c43945c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration charreaubell--demo_data-e1481c242d53578c\n",
      "Reusing dataset parquet (/Users/bellcs1/.cache/huggingface/datasets/parquet/charreaubell--demo_data-e1481c242d53578c/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3c3f5a236945468b0b3f8d1470ce2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_path = 'charreaubell/demo_data'\n",
    "demo_ds = load_dataset(ds_path, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558ecf4-e35b-4afa-a1aa-ca16e86ab432",
   "metadata": {},
   "source": [
    "# 2. Pre-process inputs\n",
    "What's a tokenizer and what does it do? Let's learn more using Huggingface's [instruction on tokenizers](https://huggingface.co/course/chapter2/4?fw=pt). Then, let's try it on our own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a7940-ae31-4efe-9398-93da6b2b0aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert-base-cased'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d994af-196d-4dba-8c1b-95a75d8fa14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define tokenizing function\n",
    "def tokenize_inputs(example):\n",
    "    return tokenizer(example['text'], truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc59e5-52b9-4449-8cda-bd94e88e980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/bellcs1/.cache/huggingface/datasets/parquet/charreaubell--demo_data-e1481c242d53578c/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121/cache-26562d7589b246b0.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dd3be256a44d9b86af65c11cd1cc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/bellcs1/.cache/huggingface/datasets/parquet/charreaubell--demo_data-e1481c242d53578c/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121/cache-799da250e96ca01b.arrow\n"
     ]
    }
   ],
   "source": [
    "#do the tokenizing using map function\n",
    "tokenized_ds = demo_ds.map(tokenize_inputs, batched=True,\n",
    "                           remove_columns = ['age', 'article_id', 'college_major',\n",
    "                                             'first_name', 'last_name', 'years_of_journalism',\n",
    "                                             'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061e38a-4e26-4294-8a3f-99f3a51de49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    valid: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label'],\n",
       "        num_rows: 12\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15655b8b-45bb-4c3c-8ad8-dcd869beda5d",
   "metadata": {},
   "source": [
    "## An aside on tokenizer functionality\n",
    "We can do many things with tokenizers to help us to tokenize our data and process it. Let's check out these outputs further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fed0e3e-a3da-45b8-9c19-8749567c96fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 107, 16409, 18220, 1106, 1143, 1254, 1725, 146, 5380, 1204, 28015, 136, 107, 1119, 1455, 119, 107, 1398, 1103, 1639, 1202, 1105, 8582, 1518, 3370, 14897, 1111, 1833, 1177, 119, 146, 1431, 1301, 1164, 1217, 2816, 3196, 1106, 28015, 1468, 1272, 146, 1221, 1115, 146, 1274, 1204, 136, 1337, 1116, 1184, 1240, 1162, 3344, 1143, 136, 102]\n",
      "\"Explain to me again why I shouldnt cheat?\" he asked. \"All the others do and nobody ever gets punished for doing so. I should go about being happy losing to cheaters because I know that I dont? Thats what youre telling me?\n"
     ]
    }
   ],
   "source": [
    "#check out input IDs\n",
    "print(tokenized_ds['train']['input_ids'][0])\n",
    "\n",
    "#compare against the text\n",
    "print(demo_ds['train']['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e1f43-5182-4094-880b-05a02c71d5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "#check out the length of the list of lists\n",
    "print(len(tokenized_ds['train']['input_ids']))\n",
    "\n",
    "#check out the length of a single element\n",
    "print(len(tokenized_ds['train']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28141308-68b3-4f8e-affe-29b89118b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '\"', 'Ex', '##plain', 'to', 'me', 'again', 'why', 'I', 'shouldn', '##t', 'cheat', '?', '\"', 'he', 'asked', '.', '\"', 'All', 'the', 'others', 'do', 'and', 'nobody', 'ever', 'gets', 'punished', 'for', 'doing', 'so', '.', 'I', 'should', 'go', 'about', 'being', 'happy', 'losing', 'to', 'cheat', '##ers', 'because', 'I', 'know', 'that', 'I', 'don', '##t', '?', 'That', '##s', 'what', 'your', '##e', 'telling', 'me', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#convert input_ids to token representation\n",
    "input0_tokens = tokenizer.convert_ids_to_tokens(tokenized_ds['train']['input_ids'][0])\n",
    "print(input0_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac4262-3678-4bf7-844a-f91d1f4d56fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] \" Explain to me again why I shouldnt cheat? \" he asked. \" All the others do and nobody ever gets punished for doing so. I should go about being happy losing to cheaters because I know that I dont? Thats what youre telling me? [SEP]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] \" Explain to me again why I shouldnt cheat? \" he asked. \" All the others do and nobody ever gets punished for doing so. I should go about being happy losing to cheaters because I know that I dont? Thats what youre telling me? [SEP]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see what this looks like as a string\n",
    "print(tokenizer.convert_tokens_to_string(input0_tokens))\n",
    "\n",
    "#another method directly from the input ids\n",
    "tokenizer.decode(tokenized_ds['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af725b84-ab00-413b-aa84-36be19efd62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inds</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28063</th>\n",
       "      <td>##egro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18645</th>\n",
       "      <td>Counsel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7819</th>\n",
       "      <td>Borough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25663</th>\n",
       "      <td>Marcia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13867</th>\n",
       "      <td>forewings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14891</th>\n",
       "      <td>embraced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>fists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28785</th>\n",
       "      <td>##』</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>emerged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>ἀ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens\n",
       "inds            \n",
       "28063     ##egro\n",
       "18645    Counsel\n",
       "7819     Borough\n",
       "25663     Marcia\n",
       "13867  forewings\n",
       "14891   embraced\n",
       "9993       fists\n",
       "28785        ##』\n",
       "5338     emerged\n",
       "768            ἀ"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#other information about tokenizer\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "#see actual tokenizer vocab (we've abbreviated here)\n",
    "#tokenizer.vocab\n",
    "pd.DataFrame({'tokens': tokenizer.vocab.keys(), 'inds': tokenizer.vocab.values()}).set_index('inds').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08845526-78fb-4096-bb4b-1ca13c7eda12",
   "metadata": {},
   "source": [
    "## An aside on dynamically padded batch size\n",
    "HF has the capacity to dynamically pad your batches such that each input is only as long as any given input in the batch. This helps with memory.You can learn more [here](https://huggingface.co/course/chapter3/2?fw=pt). For now, we'll simply instantiate a data collator and use it during training to demonstrate how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86dd09-500c-4a14-80c4-32d9a432c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da664a0-b63c-450a-ae74-bab101d55ea1",
   "metadata": {},
   "source": [
    "# 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4f917-e0c2-468c-96b7-e5227d4868b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(num_classes=5, names=['engineering', 'humanities', 'prelaw', 'premed', 'science'], names_file=None, id=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recall that the dataset carries information about the classes\n",
    "demo_ds['train'].features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347002a-6d72-4ef9-91c7-ee8996495e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the number of classes and label conversions\n",
    "no_classes = demo_ds['train'].features['label'].num_classes\n",
    "id2label = {ind:label for ind, label in enumerate(demo_ds['train'].features['label'].names)}\n",
    "label2id = {label:ind for ind, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e366960-b4a0-4ce1-8bbd-f940bb07664d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'engineering', 1: 'humanities', 2: 'prelaw', 3: 'premed', 4: 'science'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'engineering': 0, 'humanities': 1, 'prelaw': 2, 'premed': 3, 'science': 4}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check it out\n",
    "print(id2label)\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6051da-9e43-48f2-baed-65e34b6488fb",
   "metadata": {},
   "source": [
    "## Define model and task architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606be9d-e38c-48dd-9a8b-73be81ab5d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'distilbert-base-cased'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the model type and instantiate it for the task\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\",\n",
    "                                                           num_labels=no_classes,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf580c86-c17e-47d2-b63a-39df13c0d373",
   "metadata": {},
   "source": [
    "## Define settings for basic model training and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa3713-39b5-4b88-8d41-e2fe41a72ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 12\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.603900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.535800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.500800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=1.546820878982544, metrics={'train_runtime': 12.0564, 'train_samples_per_second': 2.986, 'train_steps_per_second': 0.498, 'total_flos': 928356357240.0, 'train_loss': 1.546820878982544, 'epoch': 3.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set training arguments\n",
    "training_args = TrainingArguments(\"test-trainer\",\n",
    "                                 logging_strategy='epoch')\n",
    "\n",
    "#setup training loop with arguments\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=tokenized_ds['train'],\n",
    "                  eval_dataset=tokenized_ds['valid'])\n",
    "\n",
    "#train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4997a-389d-480f-9e0b-ca39f6391399",
   "metadata": {},
   "source": [
    "### Reflect and Discuss\n",
    "* Practically speaking, how is the model performing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc468b0-0f24-42aa-8642-3e8207394f42",
   "metadata": {},
   "source": [
    "## Training with performance metrics and saving checkpoints of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53139608-67f6-417b-9d9c-9218c2f228f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79403d2587e54dfd992ae66b31115895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load a metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "#define the metric behavior\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b28b3-3eaa-438c-92b4-8931485b273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 12\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.396100</td>\n",
       "      <td>1.622399</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.340900</td>\n",
       "      <td>1.647581</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.183700</td>\n",
       "      <td>1.661458</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=1.3068776925404866, metrics={'train_runtime': 13.3811, 'train_samples_per_second': 2.69, 'train_steps_per_second': 0.448, 'total_flos': 928356357240.0, 'train_loss': 1.3068776925404866, 'epoch': 3.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set new training arguments\n",
    "training_args = TrainingArguments(\"test-trainer\",\n",
    "                                  logging_strategy = \"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  report_to='all')\n",
    "\n",
    "#setup training loop\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['valid'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "#train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8cc97f-4c52-4147-ba03-b9c2f261632b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reflect and Discuss\n",
    "* What new observations are present during model training?\n",
    "* What comments can you make on the performance of the model now?\n",
    "* What metrics are appropriate for your application?\n",
    "* Consider that model training is done in-memory (the model weights are updated in memory, but not returned), and both of our `Trainer`s trained our model `model`. After basic training from Step 9 and training from Step 10, how many epochs has the model been trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d6e99-ced3-4c06-90c0-be8291cd7fff",
   "metadata": {},
   "source": [
    "## A brief aside on resuming training from checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959b8a0-a8ae-4fbd-b619-8b154788992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the number of epochs (or steps) that you want to train for\n",
    "trainer.args.num_train_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbda02-a4e8-49c2-ad57-c8fedf149bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train some more, resuming from checkpoint\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e6f6e-a03b-475f-94a6-5aebc925164e",
   "metadata": {},
   "source": [
    "## A brief aside on performance metrics\n",
    "You may want to use other performance metrics than accuracy. Here are some [metrics available through Huggingface](https://huggingface.co/metrics). If you check out the metrics folder on the [Huggingface datasets](https://github.com/huggingface/datasets) repository, you'll be able to see what's necessary if you need to define another metric. Let's try a different metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee093f3-a6ff-4134-9009-d7eaaf8760a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    #get predictions by using index of max logit\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    #calculate classification report\n",
    "    perfs = precision_recall_fscore_support(labels, predictions, average='macro', zero_division=0)\n",
    "    perf_dict = dict(zip(['precision', 'recall', 'fscore'], perfs[:3]))\n",
    "    \n",
    "    #return dictionary\n",
    "    return perf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddbec9-7073-4dd3-a1cf-7e4213d54dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 12\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.223100</td>\n",
       "      <td>1.548289</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.124700</td>\n",
       "      <td>1.543248</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.044100</td>\n",
       "      <td>1.558989</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-trainer/checkpoint-2\n",
      "Configuration saved in test-trainer/checkpoint-2/config.json\n",
      "Model weights saved in test-trainer/checkpoint-2/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-2/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-2/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-trainer/checkpoint-4\n",
      "Configuration saved in test-trainer/checkpoint-4/config.json\n",
      "Model weights saved in test-trainer/checkpoint-4/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-4/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-4/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-trainer/checkpoint-6\n",
      "Configuration saved in test-trainer/checkpoint-6/config.json\n",
      "Model weights saved in test-trainer/checkpoint-6/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-6/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-6/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=1.1306577920913696, metrics={'train_runtime': 15.7668, 'train_samples_per_second': 2.283, 'train_steps_per_second': 0.381, 'total_flos': 928356357240.0, 'train_loss': 1.1306577920913696, 'epoch': 3.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup training loop\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['valid'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4468a7-dba9-4ed2-8dd7-8ebd79f7eeec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A brief aside on model training - TRY IT YOURSELF!\n",
    "One of several points of ambiguity when training models is how long should they train for? A way to approach this is to monitor the models and run them repeatedly, starting from the last checkpoint. Another way is through training a number of epochs (if you model trains quickly enough) and then always load the best model according to some metric at the end. Let's take a look at this.\n",
    "\n",
    "We can realize this through `TrainingArguments`! In your breakout rooms, add the parameters which will enable the following:\n",
    "1. Load the best model at the end\n",
    "2. Set the metric for using the best model to one of our evaluation metrics\n",
    "3. Examine the `greater_is_better` parameter. Do you need to change it?\n",
    "4. Change the number of training epochs to something larger.\n",
    "5. Decrease the training batch size.\n",
    "6. Decrease the eval batch size.\n",
    "7. How can you change the logging, evaluation, and save strategies to step? What else might you need to change depending on the interval of steps that you want these activities to occur?\n",
    "\n",
    "Make sure this works, so run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec97fd1-8cb3-4912-b538-0b8dcb7bc2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 12\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>1.532724</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.785400</td>\n",
       "      <td>1.607552</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.575800</td>\n",
       "      <td>1.576705</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.492100</td>\n",
       "      <td>1.553939</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.464400</td>\n",
       "      <td>1.553442</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to test-trainer/checkpoint-3\n",
      "Configuration saved in test-trainer/checkpoint-3/config.json\n",
      "Model weights saved in test-trainer/checkpoint-3/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-3/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-3/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to test-trainer/checkpoint-6\n",
      "Configuration saved in test-trainer/checkpoint-6/config.json\n",
      "Model weights saved in test-trainer/checkpoint-6/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-6/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-6/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to test-trainer/checkpoint-9\n",
      "Configuration saved in test-trainer/checkpoint-9/config.json\n",
      "Model weights saved in test-trainer/checkpoint-9/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-9/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-9/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to test-trainer/checkpoint-12\n",
      "Configuration saved in test-trainer/checkpoint-12/config.json\n",
      "Model weights saved in test-trainer/checkpoint-12/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-12/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-12/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to test-trainer/checkpoint-15\n",
      "Configuration saved in test-trainer/checkpoint-15/config.json\n",
      "Model weights saved in test-trainer/checkpoint-15/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-15/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-15/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from test-trainer/checkpoint-3 (score: 0.2).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=0.6603476365407308, metrics={'train_runtime': 27.8783, 'train_samples_per_second': 2.152, 'train_steps_per_second': 0.538, 'total_flos': 1458254300280.0, 'train_loss': 0.6603476365407308, 'epoch': 5.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set new training arguments\n",
    "training_args = TrainingArguments(\"test-trainer\",\n",
    "                                  overwrite_output_dir=True,\n",
    "                                  logging_strategy = \"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy='epoch',\n",
    "                                  load_best_model_at_end = True,\n",
    "                                  metric_for_best_model='fscore',\n",
    "                                  greater_is_better=True,\n",
    "                                  per_device_train_batch_size = 4,\n",
    "                                  per_device_eval_batch_size = 4,\n",
    "                                  num_train_epochs=5,\n",
    "                                  report_to='all')\n",
    "\n",
    "#setup training loop\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['valid'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "#train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba1189-3e5f-42ae-bfae-e79f85a439f5",
   "metadata": {},
   "source": [
    "# 4. Using trained model with `Trainer`\n",
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5bd0ec-ec3f-4f0f-b52f-70c19f6a79b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5361217260360718,\n",
       " 'eval_precision': 0.06666666666666667,\n",
       " 'eval_recall': 0.2,\n",
       " 'eval_fscore': 0.1,\n",
       " 'eval_support': None,\n",
       " 'eval_runtime': 1.0496,\n",
       " 'eval_samples_per_second': 11.433,\n",
       " 'eval_steps_per_second': 1.905,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds = trainer.evaluate(tokenized_ds['train'])\n",
    "eval_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f6ec1-24fb-43f7-a52a-1cdab4861fc5",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83797f24-bcfb-405d-9f3a-01c8b26fd7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 12\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-6.0932573e-02,  2.5887752e-01,  2.0878818e-02, -1.0038625e-02,\n",
       "         4.1899920e-02],\n",
       "       [-7.0849620e-02,  2.6222461e-01,  3.7979402e-02, -7.9516068e-02,\n",
       "         7.8266852e-02],\n",
       "       [-6.0193807e-02,  2.6336738e-01,  3.2106131e-02, -7.4920192e-02,\n",
       "         3.0398801e-02],\n",
       "       [-6.9419034e-02,  2.2632037e-01,  5.2801825e-02, -6.1321128e-02,\n",
       "         4.6445504e-02],\n",
       "       [-5.5189542e-02,  2.4058674e-01,  4.0477935e-02, -3.9512221e-02,\n",
       "         4.5452416e-03],\n",
       "       [-5.4554872e-02,  2.2160059e-01,  1.0810974e-01, -8.2729846e-02,\n",
       "         6.7920238e-04],\n",
       "       [-8.0790840e-02,  2.3849653e-01,  7.1875341e-03, -7.5528726e-02,\n",
       "         1.2777004e-01],\n",
       "       [-6.0694143e-02,  2.8064272e-01,  2.6836798e-02, -5.2206773e-02,\n",
       "         3.5131097e-02],\n",
       "       [-5.4284588e-02,  2.8006762e-01,  1.9655362e-02, -3.9448284e-02,\n",
       "         3.0258402e-02],\n",
       "       [-5.6016058e-02,  2.2985230e-01,  1.1007115e-04, -7.0286907e-02,\n",
       "         1.0518595e-01],\n",
       "       [-7.3615178e-02,  2.7098447e-01,  2.4982287e-02, -3.1357884e-02,\n",
       "         7.0124656e-02],\n",
       "       [-2.0677874e-02,  2.3338819e-01,  2.2750594e-02, -7.6885089e-02,\n",
       "         2.5874063e-02]], dtype=float32), label_ids=array([3, 1, 1, 2, 3, 2, 4, 1, 1, 4, 4, 0]), metrics={'test_loss': 1.5361217260360718, 'test_precision': 0.06666666666666667, 'test_recall': 0.2, 'test_fscore': 0.1, 'test_support': None, 'test_runtime': 1.0384, 'test_samples_per_second': 11.557, 'test_steps_per_second': 1.926})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(tokenized_ds['train'])\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad2ce4-1806-4bf3-9b68-6afa33356b82",
   "metadata": {},
   "source": [
    "# 5. Sharing and saving your model\n",
    "## Using `Trainer`\n",
    "During training and using the Trainer class, you can also upload your model directly to HuggingFace Hub as it trains. Read more about this process on the [HF course documentation](https://huggingface.co/course/chapter4/3?fw=pt).\n",
    "\n",
    "Let's check out how to do this. It's as simple as modifying our `TrainingArguments`! Don't forget to have already logged in using your authorization token or use the `use_auth_token` paramter to access your HF account. You'll need to have git-lfs installed to use this feature, so if you're on Google Colab, you can execute the line below. You can also `conda install -c conda-forge git-lfs` if you're using a conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8824a1-893c-4dce-968a-a34fcfdc5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808627ea-510d-45a4-a847-614ee7ff6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new training arguments\n",
    "training_args = TrainingArguments(\"test-trainer\",\n",
    "                                  overwrite_output_dir=True,\n",
    "                                  logging_strategy = \"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy='epoch',\n",
    "                                  push_to_hub=True,\n",
    "                                  hub_model_id='charreaubell/distilbert-magazine-classifier',\n",
    "                                  report_to='all')\n",
    "\n",
    "#setup training loop\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['valid'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "#train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4e6cf-cdc9-465d-8d6a-50b414a3e0ce",
   "metadata": {},
   "source": [
    "#### Reflect\n",
    "Visit your repository and take a look to make sure your model uploaded. Answer the following questions:\n",
    "* Where did your model save locally (directory)?\n",
    "* What are the contents of the saved model?\n",
    "* Investigate your uploaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d06945-a25e-48b2-a0e9-a9cd2b6778e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it's recommended to push the final version to HF after training completes.\n",
    "trainer.push_to_hub(commit_message='end of training 3 epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63827fe5-6f3c-4f95-a1a5-ffcdce211fcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reflect\n",
    "Visit your repository once more (you'll likely need to refresh) and check out the changes.\n",
    "* What is different from the uploads during training?\n",
    "* What do you observe about the model cards?\n",
    "\n",
    "## Fine-grained save/push access\n",
    "You can also push the model and/or tokenizer directly using the `push_to_hub` methods in their classes. You can learn more about this [in the Huggingface docs.](https://huggingface.co/course/chapter4/3?fw=pt) An example of using trainer to save your entire model locally is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4ecbe-3b1c-44f3-a96c-f691536bf78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer\n",
      "Configuration saved in test-trainer/config.json\n",
      "Model weights saved in test-trainer/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('test-trainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abc5f1-076e-4167-9c85-26d77404797f",
   "metadata": {},
   "source": [
    "# 6. Using your fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0edf1e-7ce3-450e-97dc-4abee8e6a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipeline from your classifier\n",
    "mag_classifier = pipeline('text-classification', model='test-trainer')\n",
    "\n",
    "#optionally, load from HF\n",
    "#mag_classifier = pipeline('text-classification', model='charreaubell/distilbert-magazine-classifier', use_auth_token=True)\n",
    "\n",
    "#get output\n",
    "mag_class = mag_classifier('The cat is prettier than any cat I have ever seen.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c9088-d638-4c52-aa47-096fcc4e0065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'humanities', 'score': 0.2947634756565094}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mag_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd89a2-bf3c-453a-8b53-50acce211130",
   "metadata": {},
   "source": [
    "## Reflect and discuss: Breakout Rooms\n",
    "You've successfully trained a model - great job!! Now, let's focus on what YOU need to do for your task. Using the [Transformer Notebooks](https://huggingface.co/docs/transformers/notebooks) and use the `Open in Colab` badge, explore what this task looks like. Note that even if your modality is different, you may be able to directly still use these notebooks with a few changes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
